{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports\n"
      ],
      "metadata": {
        "id": "rdxhq6vpcn0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHf0htfe0g8E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "import numpy as np\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get text and create sequences"
      ],
      "metadata": {
        "id": "AdDVzg2xcXC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_text = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_text, 'rb').read().decode(encoding='utf-8')\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = np.array(chars)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int32)\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text_as_int) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "FLYzYxSkcSRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create splits based on sequence, then prepare for training using batch & buffer size"
      ],
      "metadata": {
        "id": "NX_82qKGcrCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1]\n",
        "    target_char = chunk[-1]\n",
        "    return input_seq, target_char\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "gyvLQG8yc0oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "id": "WWADWHqHdQli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    LSTM(lstm_units, return_sequences=True),\n",
        "    LSTM(lstm_units),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "j4Y7KxKkdRr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model, store weights as checkpoint file"
      ],
      "metadata": {
        "id": "j-gXyeYNdVc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch:02d}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False,\n",
        "    save_freq='epoch'\n",
        ")\n"
      ],
      "metadata": {
        "id": "3_ZG66jgdWeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH = 10\n",
        "model.fit(dataset, epochs=EPOCH, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "qXOQS5qN0bZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "XPeikE0zzv_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text"
      ],
      "metadata": {
        "id": "dpDJbklt0GEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def latest_checkpoint():\n",
        "    weight_files = sorted(glob.glob(os.path.join(checkpoint_dir, \"*.weights.h5\")))\n",
        "    return weight_files[-1] if weight_files else None\n",
        "weights_to_load = latest_checkpoint()\n",
        "model.load_weights(weights_to_load)\n",
        "\n",
        "def generate_text(seed_text, num_generate=500, temperature=0.7):\n",
        "\n",
        "    pad_idx = 0\n",
        "    input_ids = [char2idx.get(c, pad_idx) for c in seed_text]\n",
        "    if len(input_ids) < seq_length:\n",
        "        input_ids = [pad_idx] * (seq_length - len(input_ids)) + input_ids\n",
        "    else:\n",
        "        input_ids = input_ids[-seq_length:]\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(num_generate):\n",
        "        x = np.array([input_ids], dtype=np.int32)  # shape (1, seq_length)\n",
        "        preds = model.predict(x, verbose=0)        # shape (1, vocab_size)\n",
        "        preds = preds[0].astype(np.float64)\n",
        "\n",
        "        # Temperature scaling and sampling\n",
        "        preds = np.log(preds + 1e-8) / max(1e-8, temperature)\n",
        "        exp_preds = np.exp(preds)\n",
        "        probs = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        next_id = np.random.choice(range(vocab_size), p=probs)\n",
        "        next_char = idx2char[next_id]\n",
        "        generated.append(next_char)\n",
        "\n",
        "        input_ids = input_ids[1:] + [next_id]\n",
        "    return seed_text + ''.join(generated)"
      ],
      "metadata": {
        "id": "Xrwqw_vS0Ffd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = \"MENENIUS:\"\n",
        "NUM_GENERATE = 500\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "sample = generate_text(SEED, NUM_GENERATE, TEMPERATURE)\n",
        "print(\"\\n--- Generated sample ---\\n\")\n",
        "print(sample)\n",
        "print(\"\\n--- End sample ---\\n\")"
      ],
      "metadata": {
        "id": "B26buA8t1Tdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
